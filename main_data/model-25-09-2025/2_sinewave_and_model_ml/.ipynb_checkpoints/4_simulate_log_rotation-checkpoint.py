# 4_simulate_log_rotation.py
import os
import pandas as pd
import config  # <--- (à¹ƒà¸«à¸¡à¹ˆ) Import à¹„à¸Ÿà¸¥à¹Œà¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²

# --- 1. Configuration ---
# à¸”à¸¶à¸‡à¸„à¹ˆà¸²à¸¡à¸²à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ config à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
input_base_dir = "output_cumulative_sizes"
output_base_dir = "output_log_rotation"
RETENTION_DAYS = config.RETENTION_DAYS
TIMESTAMP_FORMAT = config.TIMESTAMP_FORMAT

# (à¹ƒà¸«à¸¡à¹ˆ) à¸ªà¸£à¹‰à¸²à¸‡ List à¸‚à¸­à¸‡à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¹‚à¸”à¸¢à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
# à¹‚à¸”à¸¢à¸™à¸³à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸ˆà¸²à¸ config à¸¡à¸² à¹à¸¥à¹‰à¸§à¹à¸—à¸™à¸—à¸µà¹ˆ 'total_dir_size_bytes' à¸”à¹‰à¸§à¸¢à¸•à¸±à¸§à¹ƒà¸«à¸¡à¹ˆ
FINAL_COLUMNS = [col if col != 'total_dir_size_bytes' else 'rotated_total_dir_size_bytes' for col in config.BASE_COLUMNS]

generate_arff = True
generate_csv = True

# --- 2. Setup ---
for dataset in ["train", "test"]:
    os.makedirs(os.path.join(output_base_dir, dataset), exist_ok=True)

def save_arff(df, filename, relation_name):
    df_arff = df.copy()
    with open(filename, 'w') as f:
        f.write(f"@relation {relation_name}\n\n")
        for column in df_arff.columns:
            if column == 'time_stamp' and TIMESTAMP_FORMAT == "string":
                f.write(f"@attribute {column} DATE \"yyyy-MM-dd'T'HH:mm:ss\"\n")
            else:
                f.write(f"@attribute {column} numeric\n")
        f.write("\n@data\n")
        if 'time_stamp' in df_arff.columns and TIMESTAMP_FORMAT == "string":
             df_arff['time_stamp'] = pd.to_datetime(df_arff['time_stamp']).dt.strftime("%Y-%m-%dT%H:%M:%S")
        f.write(df_arff.to_string(header=False, index=False))

# --- 3. Main Execution ---
try:
    print(f"\nğŸš€ Starting log rotation simulation (Retention = {RETENTION_DAYS} days)...")
    processed_files, failed_files = 0, 0

    for dataset in ["train", "test"]:
        input_dir = os.path.join(input_base_dir, dataset)
        output_dir = os.path.join(output_base_dir, dataset)
        print(f"\nğŸ“ Processing dataset: '{dataset}' in '{input_dir}'")

        if not os.path.isdir(input_dir):
            print(f"   -> âš ï¸  Warning: Input directory not found. Skipping.")
            continue
        
        csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
        if not csv_files:
            print(f"   -> âš ï¸  Warning: No .csv files found.")
            continue
            
        file_count = len(csv_files)
        for i, input_filename in enumerate(csv_files):
            print(f"   -> ğŸ“„ Processing file {i+1}/{file_count}: {input_filename}")
            input_filepath = os.path.join(input_dir, input_filename)
            base_output_filename = os.path.splitext(input_filename)[0].replace("cumulative_", "rotated_")

            try:
                # à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ CSV (à¸‹à¸¶à¹ˆà¸‡à¸¡à¸µà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸•à¸²à¸¡à¸—à¸µà¹ˆ config.py à¸à¸³à¸«à¸™à¸”)
                df = pd.read_csv(input_filepath)
                if df.empty:
                    print(f"      -> ğŸŸ¡ Skipping empty file.")
                    failed_files += 1; continue
                
                # --- à¸ªà¹ˆà¸§à¸™à¸„à¸³à¸™à¸§à¸“à¸«à¸¥à¸±à¸ (Sliding Window) ---
                df['lookback_time'] = df['Time'] - (RETENTION_DAYS * 1440)
                df_merged = pd.merge_asof(
                    df.sort_values('Time'),
                    df[['Time', 'total_file_size_bytes']],
                    left_on='lookback_time', right_on='Time',
                    suffixes=('', '_lookback')
                )
                df_merged['total_file_size_bytes_lookback'] = df_merged['total_file_size_bytes_lookback'].fillna(0)
                df_merged['rotated_total_dir_size_bytes'] = df_merged['total_file_size_bytes'] - df_merged['total_file_size_bytes_lookback']
                
                final_df = df_merged[FINAL_COLUMNS]

                # --- à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ ---
                if generate_csv:
                    output_filepath_csv = os.path.join(output_dir, base_output_filename + ".csv")
                    final_df.to_csv(output_filepath_csv, index=False, float_format='%.4f')

                if generate_arff:
                    output_filepath_arff = os.path.join(output_dir, base_output_filename + ".arff")
                    relation_name = f"'{base_output_filename}_rotated'"
                    save_arff(final_df, output_filepath_arff, relation_name)

                print(f"      -> âœ… Saved rotated log size results.")
                processed_files += 1

            except Exception as e:
                print(f"      -> ğŸ”´ ERROR processing this file: {e}")
                failed_files += 1; continue
    
    print("\n" + "="*40)
    print("ğŸ‰ Log Rotation Simulation Complete! ğŸ‰")
    print(f"   Successfully processed: {processed_files} files")
    print(f"   Failed to process: {failed_files} files")
    print("="*40)

except Exception as e:
    print(f"âŒ An unexpected error occurred: {e}")